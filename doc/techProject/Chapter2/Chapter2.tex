% !TEX root = ../TechProject.tex

\graphicspath{{Chapter2/}}

\chapter{Machine Learning used in Music Recommendation Systems}



\section{The need for music recommendation systems}
Spotify, SoundCloud, Apple Music and other streaming services gives one access to a library of tens of millions songs. Music Recommendation systems are excellent at fitting a users preference whilst incorporating some level of filtering of an overwhelming amount of songs \citep{bollen_understanding_2010}. 

Music recommendation systems study users habits and taste, and with that information give out suitable recommendations. This aids the listener to discover new songs or artists they otherwise would not have found.

With this potential, a well made recommendation system could be a make or break for choosing one stream service from another. For this reason a lot of research gets poured into recommendation systems as an attempt to retain engagement and allow for company growth. 

 With recommending music, a lot of sub conscious factors come into play on what a user wants to listen to at a given time. This can range from characteristics and mood of the listener \citep{ferwerda_personality_nodate}  \citep{rentfrow_re_2003}, to what they get up to in day-to-day life \citep{gillhofer_iron_2015} \citep{wang_context-aware_2012}.  The users environment can have an affect as well \citep{kaminskas_location-aware_2013}. Observing a user made playlists also can reveal a lot on what groups of songs work for suited situations \citep{zheleva_statistical_2010} \citep{mcfee_hypergraph_2012}.
 
 A necessity for making a recommendation system suited for a type of product is taking into consideration the common place attributes of the product. Music lends itself to having a specific method due to short duration and high emotional connection. A recommendation system that uses these attributes to its advantage will be more successful than ones that don't.

\section{Collaborative Filtering}

Collaborative Filtering guesses users taste by looking at similar user-item connections. Using an explicit example, if a user rates something highly, it can provide similar suggestions from looking at other users ratings \citep{celma_recommendation_2010}.

\begin{figure}[H]
	\includegraphics[scale=0.65]{images/collaborative_filtering}
	\centering
	\caption{User-item matrix used for collaborative filtering \citep{celma_recommendation_2010}} 
	\label{fig:figure}
\end{figure}

Collaborative filtering works by taking a matrix of users and items where some form of interaction is measured. Examples of interaction's could be plays of a song, rating of a movie/product or screen time on an app. In figure 2.1, i represents items, and u is users.

The first known use of collaborative filtering is with Goldberg's Tapestry system, a mailing list filter where users collectively decide which type of emails get the most importance \citep{goldberg_using_1992}. The first instance of collaborative filtering being used was with a system called \textit{Ringo} where users would enter in music they rated and would get recommendation's pulled from similar users \citep{shardanand_social_1995}. 

\subsection{Explicit Feedback}

\textit{Ringo} and many of the first music recommendation systems that use collaborative filtering uses explicit feedback for its data. Explicit feedback is when the system asks a user for some form of measurement about the likeability of said product. \citep{celma_recommendation_2010}

An example of explicit feedback being used is in the RACOFI (Rule-Applying Collaborative Filtering) system. Ratings are used to find recommendations through collaborative filtering and then the application apply logic rules to decipher the most suitable recommendations further.\citep{anderson_racofi_2003}. Its added logic rules implicitly changes the users previous ratings. Other reccomendation systems start having similar rules including \textit{Indiscover} and Slope One \citep{celma_music_2010} \citep{lemire_slope_2007}.

\subsection{Implicit Feedback}

Recommendation systems that take only implicit feedback will focus on what the listener interacts with, rather than asking it for its opinion on the said content. The main reason implicit feedback is frowned upon is using this method doesn't give a scale of enjoyability of the content, in the context of music, just whether the user listened to said song or artist how many times. An example of this with Spotify is if another person uses there account or if they left a device on autoplay as well on mute. However, collecting implicit data is a lot easier because it just requires engagement with the said application to get valuable information. 

There is also a lot to be found with implicit data. In 2018, a music recommendation system took the time in the day which users listened to music and gained successful results \citep{sanchez-moreno_incorporating_2018}. Takama et al took this a step further including time as well as data, nationality and content features found through spotify \citep{takama_context-aware_2021} 

The two different approaches used for data finding with Collaborative Filtering is Item-based Neighbourhood or User-based neighbourhood. 

\subsection{Item Based}

\textbf{Talk about the definitions and examples of these things \citep{ricci_recommender_2011}}

Item-Based Neighbourhood is when similarites for a given item is found based on the users previous item ratings. Below shows a matrix with ratings from  $u_{2}, u_{i}$ and $u_{m-1}$. For finding similarities between $i_{i}$ and $i_{k}$, we only take $u_{2}$ and  $u_{i}$ into consideration when using item based neighbourhood because they have rated both item $i_{i}$ and $i_{k}$.  

\begin{figure}[H]
	\includegraphics[scale=0.65]{images/neigbourhood_based}
	\centering
	\caption{Matrix showing given ratings. \citep{celma_recommendation_2010}} 
\end{figure}

There are many ways to calculate how similar two items, examples being cosine similarity, Pearson correlation or adjusted cosine,. For Cosine similarity, the inner product space is taken into account for calculating its similarity, it can be described with the following equation.

\begin{equation}
		sim(i , j) = cos( \textbf{i}, \textbf{j} ) = \frac{ \textbf{ i }, \textbf{ j }}{ || i || * || j || } = \frac{ \sum_{ u \in U } r_{ u, i }, r_{ u, j }} { \sqrt{ \sum _{  u \in U } r^{2}_{ u , i}} \sqrt{ \sum _{  u \in U } r^{2}_{ u , j}}}
\end{equation}

Cosine similarity is not advised in this scenario because users usually have there own personal rangers when it comes to rating. Adjusted cosine similarity is good because it makes use of the users average rating when deciphering similarities, equation is shown below.

\begin{equation}
	sim(i , j) = \frac{ \sum_{ u \in U } ( r _{ u, i } - \bar{r} _{u} ) ( r _{ u, j} - \bar{r} _{u} ) } { \sqrt{\sum_{ u \in U } ( r _{ u, i } - \bar{r} _{u} )^2} \sqrt{\sum_{ u \in U } ( r _{ u, j } - \bar{r} _{u} )^2}}
\end{equation}

Pearson correlation gives a coefficient value from 1 to -1, 1 showing strong correlation with a positive gradient, and -1 display effective correlation with a negative change.

\begin{equation}
	sim(i , j) = \frac{ Cov( i, j) }{ \sigma _{i} \sigma _{j} } = \frac{ \sum _{ u \in U} ( r _{ u, i } - \bar{r} _{u} ) ( r _{ u, j} - \bar{r} _{u} ) } { \sqrt{\sum_{ u \in U } ( r _{ u, i } - \bar{r} _{u} )^2} \sqrt{\sum_{ u \in U } ( r _{ u, j } - \bar{r} _{u} )^2}}
\end{equation}

After the similarities are calculated, the next step is to predict how the user would rate the item in question. A way of doing this is calculated a weighted sum of the users previous item ratings. Allowing $ S^{k}(i;u)$ to equal the list of items i user u has rated, equation below shows how the predicted value is found.

\begin{equation}
	\hat{r} _{u,i} = \frac{ \sum _{ j \in S^{k}(i;u)} sim(i , j) r _{u, j}}{\sum _{j \in S^{k}(i;u)} sim(i , j)}
\end{equation}




\subsection{User Based}

User-Based Neighbourhood is when you look users who rate similar to see whether item i is similar to user u. Below shows an equation similar to the one above but instead we look through a list similar users.

\begin{equation}
	\hat{r} _{u,i} = \bar{r}_{u} + \frac{ \sum _{v \in S(u)^{k}} sim(u ,v) ( r_{v, i} - \bar{r}_{v})}{\sum _{v \in S(u)^{k}} sim(u , v)}
\end{equation}

For one to calculate $sim(u , v)$, you'd use Pearson Correlation, Cosine similarity or matrix factorisation.

Item based is when the system predict rating of user u for item i rooted from ratings of u for similar items to i. Similarity between two items are affirmed when multiple users rate them alike.

\subsection{Matrix Factorisation}

Matrix factorisation is when you take a sparse matrix and instead of storing each rating for each user, you store features that when multiplied can calculate the rating of said item by said user.

This is really useful for a sparse matrix, which is usually the case for most reccomendation systems, because it shows information that the matrix alone doesnt. Because it factorises the users ratings from features, you can see trends on what groups of users likes and doesnt like. Because it reducing the dimensionality of a given matrix, matrix factorisation requires less processing power than other methods of finding similar items. 

There are different ways of factorising a matrix. An example of one is Singular Value Decomposition (SVD). Heres the equation with U and V being the number of matrices for a given amount dimensions:

\begin{equation}
	M = U \sum V ^{T}
\end{equation}

Theres other means to compute M. Least Squares, uses a computational safe way to make sure M doesnt go to off from its original value, whilst stochastic gradient repetitively uses random bits of data to approximate U and V \citep{koren_matrix_2009}. 

When the matrix is split up, the predicted rating can be calculated from the user and item feature vectors. 

\begin{equation}
	\hat{r} _{u,i} = U _{u} . V _{i}^{T} = \sum_{f=0}^{k} U_{u,f} V_{f, i}
\end{equation}

Looking at the latent factors one can find  similar items using a cosine similarity.

\subsection{Limitations}
Despite being popular, there are a number of reasons why wouldn't use Collaborative filtering.

\textbf{Sparse Data - }Not every user has listened to every song, far from it. This means in data sets, having a sparsity of around 98-99 \% is very common.

\textbf{Grey sheep - }This is when a user has a unique taste not similar to a lot of other users, making it hard to stem recommendation's from. This a common problem with datasets that are very sparse \citep{claypool_combining_1999}.

\textbf{Cold Start - } When there is new users or items, theres little data associated with them so it makes it challanging to come up with suitable reccomendations based on either the said user or item. The term cold start refers to new items and the term refered to new users is called early raters \citep{avery_recommender_1997}.

\textbf{Popularity Bias - } Another problem with Collaborative Filtering doesnt take into account any information about the item, only users interactions with it. This means that it has a bias towards popular items.

\textbf{Feedback loops - } When users interact with items that are recommended through CF, based on previous user item interactions, it strengthens the initial recommendations more and creates a loop \citep{sanchez-moreno_incorporating_2018}.


For neighbourhood-based, recommendation can either be user-based or item-based. The Ringo system is a good example of user based recommendation's, where it assesses the taste of a user for an item using the ratings of the item from different users. These users with similar rating trends are called neighbours. 

\textbf{Talk about latest music reccomendation system that uses it (recsys one)}
\section{Content Based Filtering}

Content Based Filtering works by looking at the characteristics of a given item and see if it matches the preference of the user. Recommendations do not stem from what other users interact with, it only stems from the given information about the item.

Early uses of content based filtering were text based, becuase of its ease of information extraction. But now with advances in machine learning, more complex platform can be used for extraction like images or audio.

When finding similar items with CB, it simply looks at how similar the attributes of each items is, without introducing any subjective factor, like user behaviour, into its decision making. Thinking of an item being a vector made of its defined values of attributes, we can find the distance between each item. Its common to have these values be numerical. Common ways of caluclating the distance is Euclidean, Manhattan, Chebychev, cosine distance for vectors, and Mahalanobis distance.

\begin{equation}
	d(x,y) = \sqrt{\sum _{i=1} ^{n}(x_{i} - y_{i})^{2}}
\end{equation}

\begin{equation}
	d(x,y) = \sum _{i=1} ^{n} | x_{i} - y_{i} |
\end{equation}

\begin{equation}
	d(x,y) = man_{i} = _{1 . . n} | x_{i} - y_{i} |
\end{equation}

\begin{equation}
	d(x,y) = \sqrt{ ( x - y )^{ T } S^{ -1 } ( x - y ) }
\end{equation}

Euclidean, Manhattan and Chebychev distance are used when there is little relationship or correlation between attributes. If there is correlation, its advised to use Mahalanobis distance \citep{celma_recommendation_2010}].

When the attributes arent measured numericlly, one uses a delta function. When two attributes match it equals zero, other wise it equals 1.

\begin{equation}
	d(x,y) = 
\end{equation}


\textbf{intro of how it works}

\textbf{general history and music history}

\textbf{Talk about latest music reccomendation system that uses it (recsys one)}
\section{Clustering}
\textbf{intro of how it works}

\textbf{general history and music history}

\textbf{Talk about latest music reccomendation system that uses it (recsys one)}

\section{Convoluted Networks}
\textbf{intro of how it works}

\textbf{general history and music history}

\textbf{Talk about latest music reccomendation system that uses it (recsys one)}

% note that \Blindocument has 5 numbered levels, despite setting secnumdepth above. I (and many style guides) would suggest using no more than 3 numbered levels (incl. the chapter), with the option of a fourth unnumbered level.